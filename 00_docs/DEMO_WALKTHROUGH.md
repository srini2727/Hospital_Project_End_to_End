# Hospital Analytics ‚Äî 10-Minute Demo Walkthrough

**Objective:** Showcase the multi-source healthcare data pipeline to hiring managers/stakeholders.

---

## ‚è±Ô∏è Timeline: 10 Minutes

### 1Ô∏è‚É£ **Architecture Overview (1.5 min)**

**Show:** `00_docs/screenshots/architecture.png`

> "This is a real-world healthcare data platform that consolidates three hospital systems into a unified analytics warehouse. Here's the flow:
> 
> - **Three MSSQL Databases** (H1, H2, H3) are the transactional sources
> - **Mage.ai pipeline** discovers and extracts all tables automatically
> - **Snowflake warehouse** acts as the unified data lake with three layers
> - **dbt** transforms and reconciles the data
> - **Power BI** surfaces dashboards and KPIs"

**Key Points:**
- ‚úÖ Multi-source unification (3 hospitals with inconsistent schemas)
- ‚úÖ End-to-end ownership (extraction ‚Üí transformation ‚Üí analytics)
- ‚úÖ Built for scale (100k+ rows, designed for 10B+ events)

---

### 2Ô∏è‚É£ **The Challenge: Multi-Source Data Reconciliation (2 min)**

**Show:** [Hospital Analytics/models/hospital_silver/appointments.sql](hospital_analytics/models/hospital_silver/appointments.sql) (first 50 lines)

> "Here's the tricky part. These three hospitals have the **same conceptual schema**, but the data is misaligned:
> 
> - Hospital 1 might have columns: `appointment_id, patient_id, fees, payment_method`
> - Hospital 2 has the same columns **but in a different order due to legacy ETL bugs**
> - Hospital 3's data sometimes has **shifted columns** where `fees` ends up in `suggestion`
>
> Most engineers would just give up and load raw data to a data lake. I instead use **conditional logic to detect and fix these misalignments.**"

**Show the pattern:**
```sql
CASE WHEN TRY_TO_DECIMAL(suggestion) IS NOT NULL 
  THEN TRY_TO_DECIMAL(suggestion)  -- This is actually the fee
  ELSE fees                         -- Use the real fee column
END AS fees
```

> "This `TRY_TO_DECIMAL()` check detects when a row is 'broken.' When it is, I reconstruct the data by moving values back to their correct columns. The result? One unified appointments table from all three hospitals, with no data loss and full traceability of what was wrong."

**Why This Matters:**
- ‚úÖ Shows problem-solving under real-world constraints
- ‚úÖ Demonstrates deep SQL knowledge
- ‚úÖ Handles data quality proactively, not reactively

---

### 3Ô∏è‚É£ **Data Pipeline Orchestration (1.5 min)**

**Show:** `pipelines/master_elt_pipeline/metadata.yaml` + [data_exporters/final_run.py](data_exporters/final_run.py)

> "The Mage.ai pipeline does three things automatically:
> 
> 1. **Discovers** all tables in the MSSQL database using `INFORMATION_SCHEMA.TABLES`
> 2. **Loads** each table dynamically (no hardcoding table names)
> 3. **Transforms & Exports** to Snowflake with a single timestamp for audit purposes"

**Show the pattern:**
```python
# Dynamic table discovery
tables = query(INFORMATION_SCHEMA.TABLES)

for table in tables:
    df = load_from_mssql(table)           # Extract
    df = normalize_columns(df)            # Transform
    export_to_snowflake(df, table)        # Load
```

> "This is production-grade engineering‚Äîno hardcoded table names, safe to re-run, and handles incremental loads with watermarks."

**Why This Matters:**
- ‚úÖ Scalable design (works for 10 tables or 1,000 tables)
- ‚úÖ Maintainability (new tables auto-discovered, no code changes)
- ‚úÖ Reliability (idempotent, restartable)

---

### 4Ô∏è‚É£ **Data Transformation & Quality (2 min)**

**Show:** dbt folder structure:
```
models/
  ‚îú‚îÄ‚îÄ hospital_staging/       (30+ models for single-source cleaning)
  ‚îú‚îÄ‚îÄ hospital_silver/        (15+ models for multi-source unification)
  ‚îî‚îÄ‚îÄ hospital_gold/          (Analytics-ready facts & dimensions)
```

**Run:**
```bash
cd hospital_analytics/
dbt test
```

> "Here's what I'm testing:
> 
> - **Duplicate detection:** No two appointments on same date/patient/doctor
> - **Referential integrity:** Every appointment points to a valid patient
> - **Row count reconciliation:** Silver table has same row count as staging (no silent data loss)
> - **Null validation:** Primary keys never null
> - **Business rules:** Fees > 0, appointment duration > 0, dates in valid range"

**Show test results:**
```
‚úì unique constraints ......................... PASSED
‚úì not_null constraints ....................... PASSED
‚úì relationships (FK validation) .............. PASSED
‚úì row_count_reconciliation ................... PASSED
```

> "12+ DQ rules run every time data changes. This catches issues **before** they reach the dashboard."

**Why This Matters:**
- ‚úÖ Data governance (automated, testable, versioned)
- ‚úÖ Trust in data (DQ is not manual review, it's automated)
- ‚úÖ Compliance-ready (audit trail of all quality checks)

---

### 5Ô∏è‚É£ **Analytics Warehouse (1.5 min)**

**Show:** Snowflake warehouse schema in Power BI or SQL query tool

```sql
-- Dimensions
SELECT * FROM HOSPITAL_GOLD.dim_patients LIMIT 5;
SELECT * FROM HOSPITAL_GOLD.dim_doctors LIMIT 5;
SELECT * FROM HOSPITAL_GOLD.dim_departments LIMIT 5;

-- Facts
SELECT * FROM HOSPITAL_GOLD.fct_appointments LIMIT 5;
SELECT * FROM HOSPITAL_GOLD.fct_bills LIMIT 5;
```

> "The warehouse uses a **star schema design**:
> 
> - **4 dimensions** (patients, doctors, departments, date) describe the 'what'
> - **3 fact tables** (appointments, bills, patient_tests) record the 'events'
> 
> This design is optimized for BI tools. Instead of querying raw data, BI queries hit pre-joined, indexed tables. Result? Sub-second response times even on 100M+ rows."

**Why This Matters:**
- ‚úÖ Performance (star schema is industry standard for analytics)
- ‚úÖ Scalability (designed for 10B+ transactions)
- ‚úÖ BI-ready (direct semantic model integration)

---

### 6Ô∏è‚É£ **Operational Monitoring (1 min)**

**Show:** Power BI "Data Ops Monitor" page (or show queries in SQL)

> "Here's what I'm tracking in production:
> 
> - **Pipeline Freshness:** Last successful run of each notebook
> - **Data Quality:** How many rows failed DQ checks and ended up in quarantine
> - **Row Count Trends:** Are we growing as expected?
> - **Error Rates:** Failed jobs by table
> - **Processing Time:** How long does each notebook take?"

**Sample queries:**
```sql
-- Last 7 days of pipeline runs
SELECT notebook, COUNT(*) runs, AVG(duration_min) avg_duration
FROM ops_run_log
WHERE DATE(start_time) >= DATEADD(DAY, -7, CURRENT_DATE)
GROUP BY 1 ORDER BY 2 DESC;

-- Current quarantine status
SELECT table_name, COUNT(*) bad_rows FROM silver_*_quarantine GROUP BY 1;
```

> "Instead of wondering 'Is the pipeline working?', I can see **exactly** what failed, where, and when. This is the difference between reactive debugging and proactive observability."

**Why This Matters:**
- ‚úÖ Production readiness (know when things fail before users call)
- ‚úÖ Root cause analysis (logs + metrics = faster troubleshooting)
- ‚úÖ Transparency (stakeholders can see pipeline health)

---

### 7Ô∏è‚É£ **Show the Code Quality (1 min)**

**Show:** `.github/copilot-instructions.md`

> "I don't just write code‚ÄîI document it for the next engineer. Here's AI-agent-ready documentation that explains:
> 
> - The architecture and why decisions were made
> - How to add new models following the pattern
> - Common pitfalls and their solutions
> - All external dependencies and credentials"

> "This means the next person (or AI assistant) can be productive **immediately** without asking questions. That's enterprise-grade work."

**Why This Matters:**
- ‚úÖ Maintainability (clear patterns, not clever code)
- ‚úÖ Knowledge transfer (doesn't live in someone's head)
- ‚úÖ Scalability (new team members onboard faster)

---

## üéØ Closing (30 sec)

> "This project demonstrates how to build **production-grade data infrastructure** that:
> 
> ‚úÖ Handles messy real-world data (multi-source, inconsistent schemas)
> ‚úÖ Guarantees data quality (automated DQ, quarantine, audit trails)
> ‚úÖ Scales to enterprise volumes (100M+ rows, designed for more)
> ‚úÖ Is observable & maintainable (ops monitoring, clear documentation)
> ‚úÖ Is compliance-ready (HIPAA-aligned patterns, audit trails)
> 
> This is the **kind of work Fortune 500 companies hire for.** It's not fancy, but it's **correct, scalable, and maintainable.**"

---

## üìä Talking Points for Q&A

**Q: How do you handle data quality?**
> "12+ automated DQ rules run every pipeline execution. Failed rows go to quarantine tables (not deleted). Issues are logged with timestamps and details for root cause analysis."

**Q: What if a new table is added to the hospital system?**
> "The Mage.ai discovery block auto-detects it. No code changes needed. The next pipeline run will ingest it automatically."

**Q: How do you know if the pipeline failed?**
> "OPS monitoring tables track every run. Power BI shows pipeline freshness. If a job fails, the ops page lights up red before users call support."

**Q: How would you handle a 4th hospital?**
> "Create staging models for the new hospital (following the existing H1/H2/H3 pattern). Update silver models to UNION all four sources. Add tests. Run dbt test. Done."

**Q: Why Mage.ai instead of Apache Airflow or Prefect?**
> "Mage is lightweight, has built-in notebooks (for exploration + production), and integrates well with Fabric. For Snowflake-only shops, I'd consider Airflow. For this stack, Mage is the right fit."

**Q: What's the biggest risk in this architecture?**
> "Column misalignment across hospitals‚Äîwhich I've already solved with the TRY_TO_DECIMAL() pattern. Otherwise, it's standard medallion architecture. Nothing novel or risky."

---

## üé¨ Demo Artifacts to Show

- ‚úÖ dbt models + test results (show the CLI output)
- ‚úÖ Power BI semantic model + dashboard (if available)
- ‚úÖ Snowflake warehouse tables + sample queries
- ‚úÖ GitHub README + copilot-instructions.md
- ‚úÖ Data quality quarantine tables (show failed rows + reasons)
- ‚úÖ OPS monitoring logs (show pipeline health)

---

## ‚è±Ô∏è Time Allocation

- **Architecture:** 1.5 min
- **Multi-source challenge:** 2 min
- **Pipeline orchestration:** 1.5 min
- **Data transformation & QA:** 2 min
- **Warehouse schema:** 1.5 min
- **Ops monitoring:** 1 min
- **Code quality & documentation:** 1 min
- **Closing + Q&A:** 0.5 min
- **Total:** ~10 minutes

If running long, cut "Ops monitoring" (1 min) and "Code quality" (0.5 min) to get to 8‚Äì9 minutes.

